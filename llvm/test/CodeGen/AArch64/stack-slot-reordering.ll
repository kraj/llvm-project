; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=aarch64-linux-gnu -verify-machineinstrs < %s | FileCheck %s

; Test that our stack slot reordering optimization correctly groups
; FPR accesses with FPR and GPR accesses with GPR, not mixing them

define void @test_no_mixed_fpr_gpr_pairing(ptr %out) {
; CHECK-LABEL: test_no_mixed_fpr_gpr_pairing:
; Verify we have both GPR and FPR pairs for spills and reloads
; CHECK: stp x{{[0-9]+}}, x{{[0-9]+}}, [sp{{.*}}] {{.*}} Folded Spill
; CHECK: stp d{{[0-9]+}}, d{{[0-9]+}}, [sp{{.*}}] {{.*}} Folded Spill
; CHECK: ldp x{{[0-9]+}}, x{{[0-9]+}}, [sp{{.*}}] {{.*}} Folded Reload
; CHECK: ldp d{{[0-9]+}}, d{{[0-9]+}}, [sp{{.*}}] {{.*}} Folded Reload
entry:
  ; Load integer values that will be spilled to GPR slots
  %v0 = load i64, ptr %out, align 8
  %p1 = getelementptr i64, ptr %out, i64 1
  %v1 = load i64, ptr %p1, align 8

  ; Load floating-point values that will be spilled to FPR slots
  %p2 = getelementptr double, ptr %out, i64 2
  %f0 = load double, ptr %p2, align 8
  %p3 = getelementptr double, ptr %out, i64 3
  %f1 = load double, ptr %p3, align 8

  ; Load more integers
  %p4 = getelementptr i64, ptr %out, i64 4
  %v2 = load i64, ptr %p4, align 8
  %p5 = getelementptr i64, ptr %out, i64 5
  %v3 = load i64, ptr %p5, align 8

  ; Load more floats
  %p6 = getelementptr double, ptr %out, i64 6
  %f2 = load double, ptr %p6, align 8
  %p7 = getelementptr double, ptr %out, i64 7
  %f3 = load double, ptr %p7, align 8

  ; Force all values to be spilled
  call void asm sideeffect "", "~{x1},~{x2},~{x3},~{x4},~{x5},~{x6},~{x7},~{x8},~{x9},~{x10},~{x11},~{x12},~{x13},~{x14},~{x15},~{x16},~{x17},~{x18},~{x19},~{x20},~{x21},~{x22},~{x23},~{x24},~{x25},~{x26},~{x27},~{x28},~{d0},~{d1},~{d2},~{d3},~{d4},~{d5},~{d6},~{d7},~{d8},~{d9},~{d10},~{d11},~{d12},~{d13},~{d14},~{d15},~{d16},~{d17},~{d18},~{d19},~{d20},~{d21},~{d22},~{d23},~{d24},~{d25},~{d26},~{d27},~{d28},~{d29},~{d30},~{d31}"()

  ; Use integer pairs together
  %add01 = add i64 %v0, %v1
  store i64 %add01, ptr %out, align 8

  ; Use float pairs together
  %fadd01 = fadd double %f0, %f1
  store double %fadd01, ptr %p2, align 8

  ; Use another integer pair
  %add23 = add i64 %v2, %v3
  store i64 %add23, ptr %p1, align 8

  ; Use another float pair
  %fadd23 = fadd double %f2, %f3
  store double %fadd23, ptr %p3, align 8

  ret void
}