; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 6
; RUN: llc --mtriple=loongarch32 --mattr=+32s,+lasx < %s | FileCheck %s --check-prefixes=CHECK,LA32
; RUN: llc --mtriple=loongarch64 --mattr=+lasx < %s | FileCheck %s --check-prefixes=CHECK,LA64

define void @vmulwev_h_b(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwev_h_b:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvmulwev.h.b $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <32 x i8>, ptr %a
  %vb = load <32 x i8>, ptr %b
  %vas = shufflevector <32 x i8> %va, <32 x i8> poison, <16 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14, i32 16, i32 18, i32 20, i32 22, i32 24, i32 26, i32 28, i32 30>
  %vbs = shufflevector <32 x i8> %vb, <32 x i8> poison, <16 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14, i32 16, i32 18, i32 20, i32 22, i32 24, i32 26, i32 28, i32 30>
  %vae = sext <16 x i8> %vas to <16 x i16>
  %vbe = sext <16 x i8> %vbs to <16 x i16>
  %mul = mul <16 x i16> %vae, %vbe
  store <16 x i16> %mul, ptr %res
  ret void
}

define void @vmulwev_w_h(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwev_w_h:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvmulwev.w.h $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <16 x i16>, ptr %a
  %vb = load <16 x i16>, ptr %b
  %vas = shufflevector <16 x i16> %va, <16 x i16> poison, <8 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14>
  %vbs = shufflevector <16 x i16> %vb, <16 x i16> poison, <8 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14>
  %vae = sext <8 x i16> %vas to <8 x i32>
  %vbe = sext <8 x i16> %vbs to <8 x i32>
  %mul = mul <8 x i32> %vae, %vbe
  store <8 x i32> %mul, ptr %res
  ret void
}

define void @vmulwev_d_w(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwev_d_w:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvmulwev.d.w $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <8 x i32>, ptr %a
  %vb = load <8 x i32>, ptr %b
  %vas = shufflevector <8 x i32> %va, <8 x i32> poison, <4 x i32> <i32 0, i32 2, i32 4, i32 6>
  %vbs = shufflevector <8 x i32> %vb, <8 x i32> poison, <4 x i32> <i32 0, i32 2, i32 4, i32 6>
  %vae = sext <4 x i32> %vas to <4 x i64>
  %vbe = sext <4 x i32> %vbs to <4 x i64>
  %mul = mul <4 x i64> %vae, %vbe
  store <4 x i64> %mul, ptr %res
  ret void
}

define void @vmulwev_q_d(ptr %res, ptr %a, ptr %b) nounwind {
; LA32-LABEL: vmulwev_q_d:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    addi.w $sp, $sp, -32
; LA32-NEXT:    st.w $fp, $sp, 28 # 4-byte Folded Spill
; LA32-NEXT:    st.w $s0, $sp, 24 # 4-byte Folded Spill
; LA32-NEXT:    st.w $s1, $sp, 20 # 4-byte Folded Spill
; LA32-NEXT:    st.w $s2, $sp, 16 # 4-byte Folded Spill
; LA32-NEXT:    st.w $s3, $sp, 12 # 4-byte Folded Spill
; LA32-NEXT:    st.w $s4, $sp, 8 # 4-byte Folded Spill
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 4
; LA32-NEXT:    xvld $xr1, $a2, 0
; LA32-NEXT:    xvpickve2gr.w $a2, $xr0, 0
; LA32-NEXT:    xvpickve2gr.w $t3, $xr0, 1
; LA32-NEXT:    xvpickve2gr.w $a5, $xr0, 5
; LA32-NEXT:    xvpickve2gr.w $a3, $xr1, 4
; LA32-NEXT:    xvpickve2gr.w $a4, $xr1, 0
; LA32-NEXT:    xvpickve2gr.w $t4, $xr1, 1
; LA32-NEXT:    xvpickve2gr.w $a7, $xr1, 5
; LA32-NEXT:    srai.w $t1, $a5, 31
; LA32-NEXT:    srai.w $t5, $t3, 31
; LA32-NEXT:    srai.w $t0, $a7, 31
; LA32-NEXT:    srai.w $t6, $t4, 31
; LA32-NEXT:    mulh.wu $a6, $a2, $a4
; LA32-NEXT:    mul.w $t2, $t3, $a4
; LA32-NEXT:    add.w $a6, $t2, $a6
; LA32-NEXT:    sltu $t2, $a6, $t2
; LA32-NEXT:    mulh.wu $t7, $t3, $a4
; LA32-NEXT:    add.w $t7, $t7, $t2
; LA32-NEXT:    mul.w $t2, $a2, $t4
; LA32-NEXT:    add.w $a6, $t2, $a6
; LA32-NEXT:    sltu $t2, $a6, $t2
; LA32-NEXT:    mulh.wu $t8, $a2, $t4
; LA32-NEXT:    add.w $t2, $t8, $t2
; LA32-NEXT:    add.w $t8, $t7, $t2
; LA32-NEXT:    mul.w $fp, $t3, $t4
; LA32-NEXT:    add.w $s0, $fp, $t8
; LA32-NEXT:    mul.w $s1, $a4, $t5
; LA32-NEXT:    mul.w $s2, $t6, $a2
; LA32-NEXT:    add.w $s3, $s2, $s1
; LA32-NEXT:    add.w $t2, $s0, $s3
; LA32-NEXT:    sltu $s4, $t2, $s0
; LA32-NEXT:    sltu $fp, $s0, $fp
; LA32-NEXT:    sltu $t7, $t8, $t7
; LA32-NEXT:    mulh.wu $t8, $t3, $t4
; LA32-NEXT:    add.w $t7, $t8, $t7
; LA32-NEXT:    add.w $t7, $t7, $fp
; LA32-NEXT:    mulh.wu $t8, $a4, $t5
; LA32-NEXT:    add.w $t8, $t8, $s1
; LA32-NEXT:    mul.w $t4, $t4, $t5
; LA32-NEXT:    add.w $t4, $t8, $t4
; LA32-NEXT:    mul.w $t3, $t6, $t3
; LA32-NEXT:    mulh.wu $t5, $t6, $a2
; LA32-NEXT:    add.w $t3, $t5, $t3
; LA32-NEXT:    add.w $t3, $t3, $s2
; LA32-NEXT:    add.w $t3, $t3, $t4
; LA32-NEXT:    sltu $t4, $s3, $s2
; LA32-NEXT:    add.w $t3, $t3, $t4
; LA32-NEXT:    add.w $t3, $t7, $t3
; LA32-NEXT:    add.w $t3, $t3, $s4
; LA32-NEXT:    mulh.wu $t4, $a1, $a3
; LA32-NEXT:    mul.w $t5, $a5, $a3
; LA32-NEXT:    add.w $t4, $t5, $t4
; LA32-NEXT:    sltu $t5, $t4, $t5
; LA32-NEXT:    mulh.wu $t6, $a5, $a3
; LA32-NEXT:    add.w $t5, $t6, $t5
; LA32-NEXT:    mul.w $t6, $a1, $a7
; LA32-NEXT:    add.w $t4, $t6, $t4
; LA32-NEXT:    sltu $t6, $t4, $t6
; LA32-NEXT:    mulh.wu $t7, $a1, $a7
; LA32-NEXT:    add.w $t6, $t7, $t6
; LA32-NEXT:    add.w $t6, $t5, $t6
; LA32-NEXT:    mul.w $t7, $a5, $a7
; LA32-NEXT:    add.w $t8, $t7, $t6
; LA32-NEXT:    mul.w $fp, $a3, $t1
; LA32-NEXT:    mul.w $s0, $t0, $a1
; LA32-NEXT:    add.w $s1, $s0, $fp
; LA32-NEXT:    add.w $s2, $t8, $s1
; LA32-NEXT:    sltu $s3, $s2, $t8
; LA32-NEXT:    sltu $t7, $t8, $t7
; LA32-NEXT:    sltu $t5, $t6, $t5
; LA32-NEXT:    mulh.wu $t6, $a5, $a7
; LA32-NEXT:    add.w $t5, $t6, $t5
; LA32-NEXT:    add.w $t5, $t5, $t7
; LA32-NEXT:    mulh.wu $t6, $a3, $t1
; LA32-NEXT:    add.w $t6, $t6, $fp
; LA32-NEXT:    mul.w $a7, $a7, $t1
; LA32-NEXT:    add.w $a7, $t6, $a7
; LA32-NEXT:    mul.w $a5, $t0, $a5
; LA32-NEXT:    mulh.wu $t0, $t0, $a1
; LA32-NEXT:    add.w $a5, $t0, $a5
; LA32-NEXT:    add.w $a5, $a5, $s0
; LA32-NEXT:    add.w $a5, $a5, $a7
; LA32-NEXT:    sltu $a7, $s1, $s0
; LA32-NEXT:    add.w $a5, $a5, $a7
; LA32-NEXT:    add.w $a5, $t5, $a5
; LA32-NEXT:    add.w $a5, $a5, $s3
; LA32-NEXT:    mul.w $a2, $a2, $a4
; LA32-NEXT:    mul.w $a1, $a1, $a3
; LA32-NEXT:    st.w $a1, $a0, 16
; LA32-NEXT:    st.w $a2, $a0, 0
; LA32-NEXT:    st.w $t4, $a0, 20
; LA32-NEXT:    st.w $a6, $a0, 4
; LA32-NEXT:    st.w $s2, $a0, 24
; LA32-NEXT:    st.w $t2, $a0, 8
; LA32-NEXT:    st.w $a5, $a0, 28
; LA32-NEXT:    st.w $t3, $a0, 12
; LA32-NEXT:    ld.w $s4, $sp, 8 # 4-byte Folded Reload
; LA32-NEXT:    ld.w $s3, $sp, 12 # 4-byte Folded Reload
; LA32-NEXT:    ld.w $s2, $sp, 16 # 4-byte Folded Reload
; LA32-NEXT:    ld.w $s1, $sp, 20 # 4-byte Folded Reload
; LA32-NEXT:    ld.w $s0, $sp, 24 # 4-byte Folded Reload
; LA32-NEXT:    ld.w $fp, $sp, 28 # 4-byte Folded Reload
; LA32-NEXT:    addi.w $sp, $sp, 32
; LA32-NEXT:    ret
;
; LA64-LABEL: vmulwev_q_d:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    xvld $xr1, $a2, 0
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 2
; LA64-NEXT:    xvpickve2gr.d $a2, $xr0, 0
; LA64-NEXT:    xvpickve2gr.d $a3, $xr1, 2
; LA64-NEXT:    xvpickve2gr.d $a4, $xr1, 0
; LA64-NEXT:    mul.d $a5, $a2, $a4
; LA64-NEXT:    mulh.d $a2, $a2, $a4
; LA64-NEXT:    mul.d $a4, $a1, $a3
; LA64-NEXT:    mulh.d $a1, $a1, $a3
; LA64-NEXT:    st.d $a1, $a0, 24
; LA64-NEXT:    st.d $a4, $a0, 16
; LA64-NEXT:    st.d $a2, $a0, 8
; LA64-NEXT:    st.d $a5, $a0, 0
; LA64-NEXT:    ret
entry:
  %va = load <4 x i64>, ptr %a
  %vb = load <4 x i64>, ptr %b
  %vas = shufflevector <4 x i64> %va, <4 x i64> poison, <2 x i32> <i32 0, i32 2>
  %vbs = shufflevector <4 x i64> %vb, <4 x i64> poison, <2 x i32> <i32 0, i32 2>
  %vae = sext <2 x i64> %vas to <2 x i128>
  %vbe = sext <2 x i64> %vbs to <2 x i128>
  %mul = mul <2 x i128> %vae, %vbe
  store <2 x i128> %mul, ptr %res
  ret void
}

define void @vmulwod_h_b(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwod_h_b:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvmulwod.h.b $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <32 x i8>, ptr %a
  %vb = load <32 x i8>, ptr %b
  %vas = shufflevector <32 x i8> %va, <32 x i8> poison, <16 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15, i32 17, i32 19, i32 21, i32 23, i32 25, i32 27, i32 29, i32 31>
  %vbs = shufflevector <32 x i8> %vb, <32 x i8> poison, <16 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15, i32 17, i32 19, i32 21, i32 23, i32 25, i32 27, i32 29, i32 31>
  %vae = sext <16 x i8> %vas to <16 x i16>
  %vbe = sext <16 x i8> %vbs to <16 x i16>
  %mul = mul <16 x i16> %vae, %vbe
  store <16 x i16> %mul, ptr %res
  ret void
}

define void @vmulwod_w_h(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwod_w_h:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvmulwod.w.h $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <16 x i16>, ptr %a
  %vb = load <16 x i16>, ptr %b
  %vas = shufflevector <16 x i16> %va, <16 x i16> poison, <8 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15>
  %vbs = shufflevector <16 x i16> %vb, <16 x i16> poison, <8 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15>
  %vae = sext <8 x i16> %vas to <8 x i32>
  %vbe = sext <8 x i16> %vbs to <8 x i32>
  %mul = mul <8 x i32> %vae, %vbe
  store <8 x i32> %mul, ptr %res
  ret void
}

define void @vmulwod_d_w(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwod_d_w:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvmulwod.d.w $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <8 x i32>, ptr %a
  %vb = load <8 x i32>, ptr %b
  %vas = shufflevector <8 x i32> %va, <8 x i32> poison, <4 x i32> <i32 1, i32 3, i32 5, i32 7>
  %vbs = shufflevector <8 x i32> %vb, <8 x i32> poison, <4 x i32> <i32 1, i32 3, i32 5, i32 7>
  %vae = sext <4 x i32> %vas to <4 x i64>
  %vbe = sext <4 x i32> %vbs to <4 x i64>
  %mul = mul <4 x i64> %vae, %vbe
  store <4 x i64> %mul, ptr %res
  ret void
}

define void @vmulwod_q_d(ptr %res, ptr %a, ptr %b) nounwind {
; LA32-LABEL: vmulwod_q_d:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    addi.w $sp, $sp, -32
; LA32-NEXT:    st.w $fp, $sp, 28 # 4-byte Folded Spill
; LA32-NEXT:    st.w $s0, $sp, 24 # 4-byte Folded Spill
; LA32-NEXT:    st.w $s1, $sp, 20 # 4-byte Folded Spill
; LA32-NEXT:    st.w $s2, $sp, 16 # 4-byte Folded Spill
; LA32-NEXT:    st.w $s3, $sp, 12 # 4-byte Folded Spill
; LA32-NEXT:    st.w $s4, $sp, 8 # 4-byte Folded Spill
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 6
; LA32-NEXT:    xvld $xr1, $a2, 0
; LA32-NEXT:    xvpickve2gr.w $a2, $xr0, 2
; LA32-NEXT:    xvpickve2gr.w $t3, $xr0, 3
; LA32-NEXT:    xvpickve2gr.w $a5, $xr0, 7
; LA32-NEXT:    xvpickve2gr.w $a3, $xr1, 6
; LA32-NEXT:    xvpickve2gr.w $a4, $xr1, 2
; LA32-NEXT:    xvpickve2gr.w $t4, $xr1, 3
; LA32-NEXT:    xvpickve2gr.w $a7, $xr1, 7
; LA32-NEXT:    srai.w $t1, $a5, 31
; LA32-NEXT:    srai.w $t5, $t3, 31
; LA32-NEXT:    srai.w $t0, $a7, 31
; LA32-NEXT:    srai.w $t6, $t4, 31
; LA32-NEXT:    mulh.wu $a6, $a2, $a4
; LA32-NEXT:    mul.w $t2, $t3, $a4
; LA32-NEXT:    add.w $a6, $t2, $a6
; LA32-NEXT:    sltu $t2, $a6, $t2
; LA32-NEXT:    mulh.wu $t7, $t3, $a4
; LA32-NEXT:    add.w $t7, $t7, $t2
; LA32-NEXT:    mul.w $t2, $a2, $t4
; LA32-NEXT:    add.w $a6, $t2, $a6
; LA32-NEXT:    sltu $t2, $a6, $t2
; LA32-NEXT:    mulh.wu $t8, $a2, $t4
; LA32-NEXT:    add.w $t2, $t8, $t2
; LA32-NEXT:    add.w $t8, $t7, $t2
; LA32-NEXT:    mul.w $fp, $t3, $t4
; LA32-NEXT:    add.w $s0, $fp, $t8
; LA32-NEXT:    mul.w $s1, $a4, $t5
; LA32-NEXT:    mul.w $s2, $t6, $a2
; LA32-NEXT:    add.w $s3, $s2, $s1
; LA32-NEXT:    add.w $t2, $s0, $s3
; LA32-NEXT:    sltu $s4, $t2, $s0
; LA32-NEXT:    sltu $fp, $s0, $fp
; LA32-NEXT:    sltu $t7, $t8, $t7
; LA32-NEXT:    mulh.wu $t8, $t3, $t4
; LA32-NEXT:    add.w $t7, $t8, $t7
; LA32-NEXT:    add.w $t7, $t7, $fp
; LA32-NEXT:    mulh.wu $t8, $a4, $t5
; LA32-NEXT:    add.w $t8, $t8, $s1
; LA32-NEXT:    mul.w $t4, $t4, $t5
; LA32-NEXT:    add.w $t4, $t8, $t4
; LA32-NEXT:    mul.w $t3, $t6, $t3
; LA32-NEXT:    mulh.wu $t5, $t6, $a2
; LA32-NEXT:    add.w $t3, $t5, $t3
; LA32-NEXT:    add.w $t3, $t3, $s2
; LA32-NEXT:    add.w $t3, $t3, $t4
; LA32-NEXT:    sltu $t4, $s3, $s2
; LA32-NEXT:    add.w $t3, $t3, $t4
; LA32-NEXT:    add.w $t3, $t7, $t3
; LA32-NEXT:    add.w $t3, $t3, $s4
; LA32-NEXT:    mulh.wu $t4, $a1, $a3
; LA32-NEXT:    mul.w $t5, $a5, $a3
; LA32-NEXT:    add.w $t4, $t5, $t4
; LA32-NEXT:    sltu $t5, $t4, $t5
; LA32-NEXT:    mulh.wu $t6, $a5, $a3
; LA32-NEXT:    add.w $t5, $t6, $t5
; LA32-NEXT:    mul.w $t6, $a1, $a7
; LA32-NEXT:    add.w $t4, $t6, $t4
; LA32-NEXT:    sltu $t6, $t4, $t6
; LA32-NEXT:    mulh.wu $t7, $a1, $a7
; LA32-NEXT:    add.w $t6, $t7, $t6
; LA32-NEXT:    add.w $t6, $t5, $t6
; LA32-NEXT:    mul.w $t7, $a5, $a7
; LA32-NEXT:    add.w $t8, $t7, $t6
; LA32-NEXT:    mul.w $fp, $a3, $t1
; LA32-NEXT:    mul.w $s0, $t0, $a1
; LA32-NEXT:    add.w $s1, $s0, $fp
; LA32-NEXT:    add.w $s2, $t8, $s1
; LA32-NEXT:    sltu $s3, $s2, $t8
; LA32-NEXT:    sltu $t7, $t8, $t7
; LA32-NEXT:    sltu $t5, $t6, $t5
; LA32-NEXT:    mulh.wu $t6, $a5, $a7
; LA32-NEXT:    add.w $t5, $t6, $t5
; LA32-NEXT:    add.w $t5, $t5, $t7
; LA32-NEXT:    mulh.wu $t6, $a3, $t1
; LA32-NEXT:    add.w $t6, $t6, $fp
; LA32-NEXT:    mul.w $a7, $a7, $t1
; LA32-NEXT:    add.w $a7, $t6, $a7
; LA32-NEXT:    mul.w $a5, $t0, $a5
; LA32-NEXT:    mulh.wu $t0, $t0, $a1
; LA32-NEXT:    add.w $a5, $t0, $a5
; LA32-NEXT:    add.w $a5, $a5, $s0
; LA32-NEXT:    add.w $a5, $a5, $a7
; LA32-NEXT:    sltu $a7, $s1, $s0
; LA32-NEXT:    add.w $a5, $a5, $a7
; LA32-NEXT:    add.w $a5, $t5, $a5
; LA32-NEXT:    add.w $a5, $a5, $s3
; LA32-NEXT:    mul.w $a2, $a2, $a4
; LA32-NEXT:    mul.w $a1, $a1, $a3
; LA32-NEXT:    st.w $a1, $a0, 16
; LA32-NEXT:    st.w $a2, $a0, 0
; LA32-NEXT:    st.w $t4, $a0, 20
; LA32-NEXT:    st.w $a6, $a0, 4
; LA32-NEXT:    st.w $s2, $a0, 24
; LA32-NEXT:    st.w $t2, $a0, 8
; LA32-NEXT:    st.w $a5, $a0, 28
; LA32-NEXT:    st.w $t3, $a0, 12
; LA32-NEXT:    ld.w $s4, $sp, 8 # 4-byte Folded Reload
; LA32-NEXT:    ld.w $s3, $sp, 12 # 4-byte Folded Reload
; LA32-NEXT:    ld.w $s2, $sp, 16 # 4-byte Folded Reload
; LA32-NEXT:    ld.w $s1, $sp, 20 # 4-byte Folded Reload
; LA32-NEXT:    ld.w $s0, $sp, 24 # 4-byte Folded Reload
; LA32-NEXT:    ld.w $fp, $sp, 28 # 4-byte Folded Reload
; LA32-NEXT:    addi.w $sp, $sp, 32
; LA32-NEXT:    ret
;
; LA64-LABEL: vmulwod_q_d:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    xvld $xr1, $a2, 0
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 3
; LA64-NEXT:    xvpickve2gr.d $a2, $xr0, 1
; LA64-NEXT:    xvpickve2gr.d $a3, $xr1, 3
; LA64-NEXT:    xvpickve2gr.d $a4, $xr1, 1
; LA64-NEXT:    mul.d $a5, $a2, $a4
; LA64-NEXT:    mulh.d $a2, $a2, $a4
; LA64-NEXT:    mul.d $a4, $a1, $a3
; LA64-NEXT:    mulh.d $a1, $a1, $a3
; LA64-NEXT:    st.d $a1, $a0, 24
; LA64-NEXT:    st.d $a4, $a0, 16
; LA64-NEXT:    st.d $a2, $a0, 8
; LA64-NEXT:    st.d $a5, $a0, 0
; LA64-NEXT:    ret
entry:
  %va = load <4 x i64>, ptr %a
  %vb = load <4 x i64>, ptr %b
  %vas = shufflevector <4 x i64> %va, <4 x i64> poison, <2 x i32> <i32 1, i32 3>
  %vbs = shufflevector <4 x i64> %vb, <4 x i64> poison, <2 x i32> <i32 1, i32 3>
  %vae = sext <2 x i64> %vas to <2 x i128>
  %vbe = sext <2 x i64> %vbs to <2 x i128>
  %mul = mul <2 x i128> %vae, %vbe
  store <2 x i128> %mul, ptr %res
  ret void
}

define void @vmulwev_h_bu(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwev_h_bu:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvmulwev.h.bu $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <32 x i8>, ptr %a
  %vb = load <32 x i8>, ptr %b
  %vas = shufflevector <32 x i8> %va, <32 x i8> poison, <16 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14, i32 16, i32 18, i32 20, i32 22, i32 24, i32 26, i32 28, i32 30>
  %vbs = shufflevector <32 x i8> %vb, <32 x i8> poison, <16 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14, i32 16, i32 18, i32 20, i32 22, i32 24, i32 26, i32 28, i32 30>
  %vae = zext <16 x i8> %vas to <16 x i16>
  %vbe = zext <16 x i8> %vbs to <16 x i16>
  %mul = mul <16 x i16> %vae, %vbe
  store <16 x i16> %mul, ptr %res
  ret void
}

define void @vmulwev_w_hu(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwev_w_hu:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvmulwev.w.hu $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <16 x i16>, ptr %a
  %vb = load <16 x i16>, ptr %b
  %vas = shufflevector <16 x i16> %va, <16 x i16> poison, <8 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14>
  %vbs = shufflevector <16 x i16> %vb, <16 x i16> poison, <8 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14>
  %vae = zext <8 x i16> %vas to <8 x i32>
  %vbe = zext <8 x i16> %vbs to <8 x i32>
  %mul = mul <8 x i32> %vae, %vbe
  store <8 x i32> %mul, ptr %res
  ret void
}

define void @vmulwev_d_wu(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwev_d_wu:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvmulwev.d.wu $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <8 x i32>, ptr %a
  %vb = load <8 x i32>, ptr %b
  %vas = shufflevector <8 x i32> %va, <8 x i32> poison, <4 x i32> <i32 0, i32 2, i32 4, i32 6>
  %vbs = shufflevector <8 x i32> %vb, <8 x i32> poison, <4 x i32> <i32 0, i32 2, i32 4, i32 6>
  %vae = zext <4 x i32> %vas to <4 x i64>
  %vbe = zext <4 x i32> %vbs to <4 x i64>
  %mul = mul <4 x i64> %vae, %vbe
  store <4 x i64> %mul, ptr %res
  ret void
}

define void @vmulwev_q_du(ptr %res, ptr %a, ptr %b) nounwind {
; LA32-LABEL: vmulwev_q_du:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 5
; LA32-NEXT:    xvld $xr1, $a2, 0
; LA32-NEXT:    xvpickve2gr.w $a2, $xr0, 4
; LA32-NEXT:    xvpickve2gr.w $a3, $xr0, 1
; LA32-NEXT:    xvpickve2gr.w $a4, $xr0, 0
; LA32-NEXT:    xvpickve2gr.w $a5, $xr1, 5
; LA32-NEXT:    xvpickve2gr.w $a6, $xr1, 4
; LA32-NEXT:    xvpickve2gr.w $a7, $xr1, 1
; LA32-NEXT:    xvpickve2gr.w $t0, $xr1, 0
; LA32-NEXT:    mulh.wu $t1, $a4, $t0
; LA32-NEXT:    mul.w $t2, $a3, $t0
; LA32-NEXT:    add.w $t1, $t2, $t1
; LA32-NEXT:    sltu $t2, $t1, $t2
; LA32-NEXT:    mulh.wu $t3, $a3, $t0
; LA32-NEXT:    add.w $t2, $t3, $t2
; LA32-NEXT:    mul.w $t3, $a4, $a7
; LA32-NEXT:    add.w $t1, $t3, $t1
; LA32-NEXT:    sltu $t3, $t1, $t3
; LA32-NEXT:    mulh.wu $t4, $a4, $a7
; LA32-NEXT:    add.w $t3, $t4, $t3
; LA32-NEXT:    add.w $t3, $t2, $t3
; LA32-NEXT:    mul.w $t4, $a3, $a7
; LA32-NEXT:    add.w $t5, $t4, $t3
; LA32-NEXT:    sltu $t4, $t5, $t4
; LA32-NEXT:    sltu $t2, $t3, $t2
; LA32-NEXT:    mulh.wu $a3, $a3, $a7
; LA32-NEXT:    add.w $a3, $a3, $t2
; LA32-NEXT:    add.w $a3, $a3, $t4
; LA32-NEXT:    mulh.wu $a7, $a2, $a6
; LA32-NEXT:    mul.w $t2, $a1, $a6
; LA32-NEXT:    add.w $a7, $t2, $a7
; LA32-NEXT:    sltu $t2, $a7, $t2
; LA32-NEXT:    mulh.wu $t3, $a1, $a6
; LA32-NEXT:    add.w $t2, $t3, $t2
; LA32-NEXT:    mul.w $t3, $a2, $a5
; LA32-NEXT:    add.w $a7, $t3, $a7
; LA32-NEXT:    sltu $t3, $a7, $t3
; LA32-NEXT:    mulh.wu $t4, $a2, $a5
; LA32-NEXT:    add.w $t3, $t4, $t3
; LA32-NEXT:    add.w $t3, $t2, $t3
; LA32-NEXT:    mul.w $t4, $a1, $a5
; LA32-NEXT:    add.w $t6, $t4, $t3
; LA32-NEXT:    sltu $t4, $t6, $t4
; LA32-NEXT:    sltu $t2, $t3, $t2
; LA32-NEXT:    mulh.wu $a1, $a1, $a5
; LA32-NEXT:    add.w $a1, $a1, $t2
; LA32-NEXT:    add.w $a1, $a1, $t4
; LA32-NEXT:    mul.w $a4, $a4, $t0
; LA32-NEXT:    mul.w $a2, $a2, $a6
; LA32-NEXT:    st.w $a2, $a0, 16
; LA32-NEXT:    st.w $a4, $a0, 0
; LA32-NEXT:    st.w $a7, $a0, 20
; LA32-NEXT:    st.w $t1, $a0, 4
; LA32-NEXT:    st.w $t6, $a0, 24
; LA32-NEXT:    st.w $t5, $a0, 8
; LA32-NEXT:    st.w $a1, $a0, 28
; LA32-NEXT:    st.w $a3, $a0, 12
; LA32-NEXT:    ret
;
; LA64-LABEL: vmulwev_q_du:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    xvld $xr1, $a2, 0
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 2
; LA64-NEXT:    xvpickve2gr.d $a2, $xr0, 0
; LA64-NEXT:    xvpickve2gr.d $a3, $xr1, 2
; LA64-NEXT:    xvpickve2gr.d $a4, $xr1, 0
; LA64-NEXT:    mul.d $a5, $a2, $a4
; LA64-NEXT:    mulh.du $a2, $a2, $a4
; LA64-NEXT:    mul.d $a4, $a1, $a3
; LA64-NEXT:    mulh.du $a1, $a1, $a3
; LA64-NEXT:    st.d $a1, $a0, 24
; LA64-NEXT:    st.d $a4, $a0, 16
; LA64-NEXT:    st.d $a2, $a0, 8
; LA64-NEXT:    st.d $a5, $a0, 0
; LA64-NEXT:    ret
entry:
  %va = load <4 x i64>, ptr %a
  %vb = load <4 x i64>, ptr %b
  %vas = shufflevector <4 x i64> %va, <4 x i64> poison, <2 x i32> <i32 0, i32 2>
  %vbs = shufflevector <4 x i64> %vb, <4 x i64> poison, <2 x i32> <i32 0, i32 2>
  %vae = zext <2 x i64> %vas to <2 x i128>
  %vbe = zext <2 x i64> %vbs to <2 x i128>
  %mul = mul <2 x i128> %vae, %vbe
  store <2 x i128> %mul, ptr %res
  ret void
}

define void @vmulwod_h_bu(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwod_h_bu:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvmulwod.h.bu $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <32 x i8>, ptr %a
  %vb = load <32 x i8>, ptr %b
  %vas = shufflevector <32 x i8> %va, <32 x i8> poison, <16 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15, i32 17, i32 19, i32 21, i32 23, i32 25, i32 27, i32 29, i32 31>
  %vbs = shufflevector <32 x i8> %vb, <32 x i8> poison, <16 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15, i32 17, i32 19, i32 21, i32 23, i32 25, i32 27, i32 29, i32 31>
  %vae = zext <16 x i8> %vas to <16 x i16>
  %vbe = zext <16 x i8> %vbs to <16 x i16>
  %mul = mul <16 x i16> %vae, %vbe
  store <16 x i16> %mul, ptr %res
  ret void
}

define void @vmulwod_w_hu(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwod_w_hu:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvmulwod.w.hu $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <16 x i16>, ptr %a
  %vb = load <16 x i16>, ptr %b
  %vas = shufflevector <16 x i16> %va, <16 x i16> poison, <8 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15>
  %vbs = shufflevector <16 x i16> %vb, <16 x i16> poison, <8 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15>
  %vae = zext <8 x i16> %vas to <8 x i32>
  %vbe = zext <8 x i16> %vbs to <8 x i32>
  %mul = mul <8 x i32> %vae, %vbe
  store <8 x i32> %mul, ptr %res
  ret void
}

define void @vmulwod_d_wu(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwod_d_wu:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvmulwod.d.wu $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <8 x i32>, ptr %a
  %vb = load <8 x i32>, ptr %b
  %vas = shufflevector <8 x i32> %va, <8 x i32> poison, <4 x i32> <i32 1, i32 3, i32 5, i32 7>
  %vbs = shufflevector <8 x i32> %vb, <8 x i32> poison, <4 x i32> <i32 1, i32 3, i32 5, i32 7>
  %vae = zext <4 x i32> %vas to <4 x i64>
  %vbe = zext <4 x i32> %vbs to <4 x i64>
  %mul = mul <4 x i64> %vae, %vbe
  store <4 x i64> %mul, ptr %res
  ret void
}

define void @vmulwod_q_du(ptr %res, ptr %a, ptr %b) nounwind {
; LA32-LABEL: vmulwod_q_du:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 7
; LA32-NEXT:    xvld $xr1, $a2, 0
; LA32-NEXT:    xvpickve2gr.w $a2, $xr0, 6
; LA32-NEXT:    xvpickve2gr.w $a3, $xr0, 3
; LA32-NEXT:    xvpickve2gr.w $a4, $xr0, 2
; LA32-NEXT:    xvpickve2gr.w $a5, $xr1, 7
; LA32-NEXT:    xvpickve2gr.w $a6, $xr1, 6
; LA32-NEXT:    xvpickve2gr.w $a7, $xr1, 3
; LA32-NEXT:    xvpickve2gr.w $t0, $xr1, 2
; LA32-NEXT:    mulh.wu $t1, $a4, $t0
; LA32-NEXT:    mul.w $t2, $a3, $t0
; LA32-NEXT:    add.w $t1, $t2, $t1
; LA32-NEXT:    sltu $t2, $t1, $t2
; LA32-NEXT:    mulh.wu $t3, $a3, $t0
; LA32-NEXT:    add.w $t2, $t3, $t2
; LA32-NEXT:    mul.w $t3, $a4, $a7
; LA32-NEXT:    add.w $t1, $t3, $t1
; LA32-NEXT:    sltu $t3, $t1, $t3
; LA32-NEXT:    mulh.wu $t4, $a4, $a7
; LA32-NEXT:    add.w $t3, $t4, $t3
; LA32-NEXT:    add.w $t3, $t2, $t3
; LA32-NEXT:    mul.w $t4, $a3, $a7
; LA32-NEXT:    add.w $t5, $t4, $t3
; LA32-NEXT:    sltu $t4, $t5, $t4
; LA32-NEXT:    sltu $t2, $t3, $t2
; LA32-NEXT:    mulh.wu $a3, $a3, $a7
; LA32-NEXT:    add.w $a3, $a3, $t2
; LA32-NEXT:    add.w $a3, $a3, $t4
; LA32-NEXT:    mulh.wu $a7, $a2, $a6
; LA32-NEXT:    mul.w $t2, $a1, $a6
; LA32-NEXT:    add.w $a7, $t2, $a7
; LA32-NEXT:    sltu $t2, $a7, $t2
; LA32-NEXT:    mulh.wu $t3, $a1, $a6
; LA32-NEXT:    add.w $t2, $t3, $t2
; LA32-NEXT:    mul.w $t3, $a2, $a5
; LA32-NEXT:    add.w $a7, $t3, $a7
; LA32-NEXT:    sltu $t3, $a7, $t3
; LA32-NEXT:    mulh.wu $t4, $a2, $a5
; LA32-NEXT:    add.w $t3, $t4, $t3
; LA32-NEXT:    add.w $t3, $t2, $t3
; LA32-NEXT:    mul.w $t4, $a1, $a5
; LA32-NEXT:    add.w $t6, $t4, $t3
; LA32-NEXT:    sltu $t4, $t6, $t4
; LA32-NEXT:    sltu $t2, $t3, $t2
; LA32-NEXT:    mulh.wu $a1, $a1, $a5
; LA32-NEXT:    add.w $a1, $a1, $t2
; LA32-NEXT:    add.w $a1, $a1, $t4
; LA32-NEXT:    mul.w $a4, $a4, $t0
; LA32-NEXT:    mul.w $a2, $a2, $a6
; LA32-NEXT:    st.w $a2, $a0, 16
; LA32-NEXT:    st.w $a4, $a0, 0
; LA32-NEXT:    st.w $a7, $a0, 20
; LA32-NEXT:    st.w $t1, $a0, 4
; LA32-NEXT:    st.w $t6, $a0, 24
; LA32-NEXT:    st.w $t5, $a0, 8
; LA32-NEXT:    st.w $a1, $a0, 28
; LA32-NEXT:    st.w $a3, $a0, 12
; LA32-NEXT:    ret
;
; LA64-LABEL: vmulwod_q_du:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    xvld $xr1, $a2, 0
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 3
; LA64-NEXT:    xvpickve2gr.d $a2, $xr0, 1
; LA64-NEXT:    xvpickve2gr.d $a3, $xr1, 3
; LA64-NEXT:    xvpickve2gr.d $a4, $xr1, 1
; LA64-NEXT:    mul.d $a5, $a2, $a4
; LA64-NEXT:    mulh.du $a2, $a2, $a4
; LA64-NEXT:    mul.d $a4, $a1, $a3
; LA64-NEXT:    mulh.du $a1, $a1, $a3
; LA64-NEXT:    st.d $a1, $a0, 24
; LA64-NEXT:    st.d $a4, $a0, 16
; LA64-NEXT:    st.d $a2, $a0, 8
; LA64-NEXT:    st.d $a5, $a0, 0
; LA64-NEXT:    ret
entry:
  %va = load <4 x i64>, ptr %a
  %vb = load <4 x i64>, ptr %b
  %vas = shufflevector <4 x i64> %va, <4 x i64> poison, <2 x i32> <i32 1, i32 3>
  %vbs = shufflevector <4 x i64> %vb, <4 x i64> poison, <2 x i32> <i32 1, i32 3>
  %vae = zext <2 x i64> %vas to <2 x i128>
  %vbe = zext <2 x i64> %vbs to <2 x i128>
  %mul = mul <2 x i128> %vae, %vbe
  store <2 x i128> %mul, ptr %res
  ret void
}

define void @vmulwev_h_bu_b(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwev_h_bu_b:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvmulwev.h.bu.b $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <32 x i8>, ptr %a
  %vb = load <32 x i8>, ptr %b
  %vas = shufflevector <32 x i8> %va, <32 x i8> poison, <16 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14, i32 16, i32 18, i32 20, i32 22, i32 24, i32 26, i32 28, i32 30>
  %vbs = shufflevector <32 x i8> %vb, <32 x i8> poison, <16 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14, i32 16, i32 18, i32 20, i32 22, i32 24, i32 26, i32 28, i32 30>
  %vae = zext <16 x i8> %vas to <16 x i16>
  %vbe = sext <16 x i8> %vbs to <16 x i16>
  %mul = mul <16 x i16> %vae, %vbe
  store <16 x i16> %mul, ptr %res
  ret void
}

define void @vmulwev_w_hu_h(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwev_w_hu_h:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvmulwev.w.hu.h $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <16 x i16>, ptr %a
  %vb = load <16 x i16>, ptr %b
  %vas = shufflevector <16 x i16> %va, <16 x i16> poison, <8 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14>
  %vbs = shufflevector <16 x i16> %vb, <16 x i16> poison, <8 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14>
  %vae = zext <8 x i16> %vas to <8 x i32>
  %vbe = sext <8 x i16> %vbs to <8 x i32>
  %mul = mul <8 x i32> %vae, %vbe
  store <8 x i32> %mul, ptr %res
  ret void
}

define void @vmulwev_d_wu_w(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwev_d_wu_w:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvmulwev.d.wu.w $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <8 x i32>, ptr %a
  %vb = load <8 x i32>, ptr %b
  %vas = shufflevector <8 x i32> %va, <8 x i32> poison, <4 x i32> <i32 0, i32 2, i32 4, i32 6>
  %vbs = shufflevector <8 x i32> %vb, <8 x i32> poison, <4 x i32> <i32 0, i32 2, i32 4, i32 6>
  %vae = zext <4 x i32> %vas to <4 x i64>
  %vbe = sext <4 x i32> %vbs to <4 x i64>
  %mul = mul <4 x i64> %vae, %vbe
  store <4 x i64> %mul, ptr %res
  ret void
}

define void @vmulwev_q_du_d(ptr %res, ptr %a, ptr %b) nounwind {
; LA32-LABEL: vmulwev_q_du_d:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    addi.w $sp, $sp, -16
; LA32-NEXT:    st.w $fp, $sp, 12 # 4-byte Folded Spill
; LA32-NEXT:    st.w $s0, $sp, 8 # 4-byte Folded Spill
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    xvpickve2gr.w $a3, $xr0, 5
; LA32-NEXT:    xvld $xr1, $a2, 0
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 4
; LA32-NEXT:    xvpickve2gr.w $a6, $xr0, 1
; LA32-NEXT:    xvpickve2gr.w $a2, $xr0, 0
; LA32-NEXT:    xvpickve2gr.w $a4, $xr1, 4
; LA32-NEXT:    xvpickve2gr.w $a5, $xr1, 0
; LA32-NEXT:    xvpickve2gr.w $a7, $xr1, 1
; LA32-NEXT:    xvpickve2gr.w $t0, $xr1, 5
; LA32-NEXT:    srai.w $t1, $t0, 31
; LA32-NEXT:    srai.w $t2, $a7, 31
; LA32-NEXT:    mulh.wu $t3, $a2, $a5
; LA32-NEXT:    mul.w $t4, $a6, $a5
; LA32-NEXT:    add.w $t3, $t4, $t3
; LA32-NEXT:    sltu $t4, $t3, $t4
; LA32-NEXT:    mulh.wu $t5, $a6, $a5
; LA32-NEXT:    add.w $t4, $t5, $t4
; LA32-NEXT:    mul.w $t5, $a2, $a7
; LA32-NEXT:    add.w $t3, $t5, $t3
; LA32-NEXT:    sltu $t5, $t3, $t5
; LA32-NEXT:    mulh.wu $t6, $a2, $a7
; LA32-NEXT:    add.w $t5, $t6, $t5
; LA32-NEXT:    add.w $t5, $t4, $t5
; LA32-NEXT:    mul.w $t6, $a6, $a7
; LA32-NEXT:    add.w $t7, $t6, $t5
; LA32-NEXT:    mul.w $t8, $t2, $a2
; LA32-NEXT:    add.w $fp, $t7, $t8
; LA32-NEXT:    sltu $s0, $fp, $t7
; LA32-NEXT:    sltu $t6, $t7, $t6
; LA32-NEXT:    sltu $t4, $t5, $t4
; LA32-NEXT:    mulh.wu $a7, $a6, $a7
; LA32-NEXT:    add.w $a7, $a7, $t4
; LA32-NEXT:    add.w $a7, $a7, $t6
; LA32-NEXT:    mul.w $a6, $t2, $a6
; LA32-NEXT:    mulh.wu $t2, $t2, $a2
; LA32-NEXT:    add.w $a6, $t2, $a6
; LA32-NEXT:    add.w $a6, $a6, $t8
; LA32-NEXT:    add.w $a6, $a7, $a6
; LA32-NEXT:    add.w $a6, $a6, $s0
; LA32-NEXT:    mulh.wu $a7, $a1, $a4
; LA32-NEXT:    mul.w $t2, $a3, $a4
; LA32-NEXT:    add.w $a7, $t2, $a7
; LA32-NEXT:    sltu $t2, $a7, $t2
; LA32-NEXT:    mulh.wu $t4, $a3, $a4
; LA32-NEXT:    add.w $t2, $t4, $t2
; LA32-NEXT:    mul.w $t4, $a1, $t0
; LA32-NEXT:    add.w $a7, $t4, $a7
; LA32-NEXT:    sltu $t4, $a7, $t4
; LA32-NEXT:    mulh.wu $t5, $a1, $t0
; LA32-NEXT:    add.w $t4, $t5, $t4
; LA32-NEXT:    add.w $t4, $t2, $t4
; LA32-NEXT:    mul.w $t5, $a3, $t0
; LA32-NEXT:    add.w $t6, $t5, $t4
; LA32-NEXT:    mul.w $t7, $t1, $a1
; LA32-NEXT:    add.w $t8, $t6, $t7
; LA32-NEXT:    sltu $s0, $t8, $t6
; LA32-NEXT:    sltu $t5, $t6, $t5
; LA32-NEXT:    sltu $t2, $t4, $t2
; LA32-NEXT:    mulh.wu $t0, $a3, $t0
; LA32-NEXT:    add.w $t0, $t0, $t2
; LA32-NEXT:    add.w $t0, $t0, $t5
; LA32-NEXT:    mul.w $a3, $t1, $a3
; LA32-NEXT:    mulh.wu $t1, $t1, $a1
; LA32-NEXT:    add.w $a3, $t1, $a3
; LA32-NEXT:    add.w $a3, $a3, $t7
; LA32-NEXT:    add.w $a3, $t0, $a3
; LA32-NEXT:    add.w $a3, $a3, $s0
; LA32-NEXT:    mul.w $a2, $a2, $a5
; LA32-NEXT:    mul.w $a1, $a1, $a4
; LA32-NEXT:    st.w $a1, $a0, 16
; LA32-NEXT:    st.w $a2, $a0, 0
; LA32-NEXT:    st.w $a7, $a0, 20
; LA32-NEXT:    st.w $t3, $a0, 4
; LA32-NEXT:    st.w $t8, $a0, 24
; LA32-NEXT:    st.w $fp, $a0, 8
; LA32-NEXT:    st.w $a3, $a0, 28
; LA32-NEXT:    st.w $a6, $a0, 12
; LA32-NEXT:    ld.w $s0, $sp, 8 # 4-byte Folded Reload
; LA32-NEXT:    ld.w $fp, $sp, 12 # 4-byte Folded Reload
; LA32-NEXT:    addi.w $sp, $sp, 16
; LA32-NEXT:    ret
;
; LA64-LABEL: vmulwev_q_du_d:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    xvld $xr1, $a2, 0
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 2
; LA64-NEXT:    xvpickve2gr.d $a2, $xr0, 0
; LA64-NEXT:    xvpickve2gr.d $a3, $xr1, 0
; LA64-NEXT:    xvpickve2gr.d $a4, $xr1, 2
; LA64-NEXT:    srai.d $a5, $a4, 63
; LA64-NEXT:    srai.d $a6, $a3, 63
; LA64-NEXT:    mulh.du $a7, $a2, $a3
; LA64-NEXT:    mul.d $a6, $a2, $a6
; LA64-NEXT:    add.d $a6, $a7, $a6
; LA64-NEXT:    mulh.du $a7, $a1, $a4
; LA64-NEXT:    mul.d $a5, $a1, $a5
; LA64-NEXT:    add.d $a5, $a7, $a5
; LA64-NEXT:    mul.d $a2, $a2, $a3
; LA64-NEXT:    mul.d $a1, $a1, $a4
; LA64-NEXT:    st.d $a1, $a0, 16
; LA64-NEXT:    st.d $a2, $a0, 0
; LA64-NEXT:    st.d $a5, $a0, 24
; LA64-NEXT:    st.d $a6, $a0, 8
; LA64-NEXT:    ret
entry:
  %va = load <4 x i64>, ptr %a
  %vb = load <4 x i64>, ptr %b
  %vas = shufflevector <4 x i64> %va, <4 x i64> poison, <2 x i32> <i32 0, i32 2>
  %vbs = shufflevector <4 x i64> %vb, <4 x i64> poison, <2 x i32> <i32 0, i32 2>
  %vae = zext <2 x i64> %vas to <2 x i128>
  %vbe = sext <2 x i64> %vbs to <2 x i128>
  %mul = mul <2 x i128> %vae, %vbe
  store <2 x i128> %mul, ptr %res
  ret void
}

define void @vmulwod_h_bu_b(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwod_h_bu_b:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvmulwod.h.bu.b $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <32 x i8>, ptr %a
  %vb = load <32 x i8>, ptr %b
  %vas = shufflevector <32 x i8> %va, <32 x i8> poison, <16 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15, i32 17, i32 19, i32 21, i32 23, i32 25, i32 27, i32 29, i32 31>
  %vbs = shufflevector <32 x i8> %vb, <32 x i8> poison, <16 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15, i32 17, i32 19, i32 21, i32 23, i32 25, i32 27, i32 29, i32 31>
  %vae = zext <16 x i8> %vas to <16 x i16>
  %vbe = sext <16 x i8> %vbs to <16 x i16>
  %mul = mul <16 x i16> %vae, %vbe
  store <16 x i16> %mul, ptr %res
  ret void
}

define void @vmulwod_w_hu_h(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwod_w_hu_h:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvmulwod.w.hu.h $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <16 x i16>, ptr %a
  %vb = load <16 x i16>, ptr %b
  %vas = shufflevector <16 x i16> %va, <16 x i16> poison, <8 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15>
  %vbs = shufflevector <16 x i16> %vb, <16 x i16> poison, <8 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15>
  %vae = zext <8 x i16> %vas to <8 x i32>
  %vbe = sext <8 x i16> %vbs to <8 x i32>
  %mul = mul <8 x i32> %vae, %vbe
  store <8 x i32> %mul, ptr %res
  ret void
}

define void @vmulwod_d_wu_w(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwod_d_wu_w:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvmulwod.d.wu.w $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <8 x i32>, ptr %a
  %vb = load <8 x i32>, ptr %b
  %vas = shufflevector <8 x i32> %va, <8 x i32> poison, <4 x i32> <i32 1, i32 3, i32 5, i32 7>
  %vbs = shufflevector <8 x i32> %vb, <8 x i32> poison, <4 x i32> <i32 1, i32 3, i32 5, i32 7>
  %vae = zext <4 x i32> %vas to <4 x i64>
  %vbe = sext <4 x i32> %vbs to <4 x i64>
  %mul = mul <4 x i64> %vae, %vbe
  store <4 x i64> %mul, ptr %res
  ret void
}

define void @vmulwod_q_du_d(ptr %res, ptr %a, ptr %b) nounwind {
; LA32-LABEL: vmulwod_q_du_d:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    addi.w $sp, $sp, -16
; LA32-NEXT:    st.w $fp, $sp, 12 # 4-byte Folded Spill
; LA32-NEXT:    st.w $s0, $sp, 8 # 4-byte Folded Spill
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    xvpickve2gr.w $a3, $xr0, 7
; LA32-NEXT:    xvld $xr1, $a2, 0
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 6
; LA32-NEXT:    xvpickve2gr.w $a6, $xr0, 3
; LA32-NEXT:    xvpickve2gr.w $a2, $xr0, 2
; LA32-NEXT:    xvpickve2gr.w $a4, $xr1, 6
; LA32-NEXT:    xvpickve2gr.w $a5, $xr1, 2
; LA32-NEXT:    xvpickve2gr.w $a7, $xr1, 3
; LA32-NEXT:    xvpickve2gr.w $t0, $xr1, 7
; LA32-NEXT:    srai.w $t1, $t0, 31
; LA32-NEXT:    srai.w $t2, $a7, 31
; LA32-NEXT:    mulh.wu $t3, $a2, $a5
; LA32-NEXT:    mul.w $t4, $a6, $a5
; LA32-NEXT:    add.w $t3, $t4, $t3
; LA32-NEXT:    sltu $t4, $t3, $t4
; LA32-NEXT:    mulh.wu $t5, $a6, $a5
; LA32-NEXT:    add.w $t4, $t5, $t4
; LA32-NEXT:    mul.w $t5, $a2, $a7
; LA32-NEXT:    add.w $t3, $t5, $t3
; LA32-NEXT:    sltu $t5, $t3, $t5
; LA32-NEXT:    mulh.wu $t6, $a2, $a7
; LA32-NEXT:    add.w $t5, $t6, $t5
; LA32-NEXT:    add.w $t5, $t4, $t5
; LA32-NEXT:    mul.w $t6, $a6, $a7
; LA32-NEXT:    add.w $t7, $t6, $t5
; LA32-NEXT:    mul.w $t8, $t2, $a2
; LA32-NEXT:    add.w $fp, $t7, $t8
; LA32-NEXT:    sltu $s0, $fp, $t7
; LA32-NEXT:    sltu $t6, $t7, $t6
; LA32-NEXT:    sltu $t4, $t5, $t4
; LA32-NEXT:    mulh.wu $a7, $a6, $a7
; LA32-NEXT:    add.w $a7, $a7, $t4
; LA32-NEXT:    add.w $a7, $a7, $t6
; LA32-NEXT:    mul.w $a6, $t2, $a6
; LA32-NEXT:    mulh.wu $t2, $t2, $a2
; LA32-NEXT:    add.w $a6, $t2, $a6
; LA32-NEXT:    add.w $a6, $a6, $t8
; LA32-NEXT:    add.w $a6, $a7, $a6
; LA32-NEXT:    add.w $a6, $a6, $s0
; LA32-NEXT:    mulh.wu $a7, $a1, $a4
; LA32-NEXT:    mul.w $t2, $a3, $a4
; LA32-NEXT:    add.w $a7, $t2, $a7
; LA32-NEXT:    sltu $t2, $a7, $t2
; LA32-NEXT:    mulh.wu $t4, $a3, $a4
; LA32-NEXT:    add.w $t2, $t4, $t2
; LA32-NEXT:    mul.w $t4, $a1, $t0
; LA32-NEXT:    add.w $a7, $t4, $a7
; LA32-NEXT:    sltu $t4, $a7, $t4
; LA32-NEXT:    mulh.wu $t5, $a1, $t0
; LA32-NEXT:    add.w $t4, $t5, $t4
; LA32-NEXT:    add.w $t4, $t2, $t4
; LA32-NEXT:    mul.w $t5, $a3, $t0
; LA32-NEXT:    add.w $t6, $t5, $t4
; LA32-NEXT:    mul.w $t7, $t1, $a1
; LA32-NEXT:    add.w $t8, $t6, $t7
; LA32-NEXT:    sltu $s0, $t8, $t6
; LA32-NEXT:    sltu $t5, $t6, $t5
; LA32-NEXT:    sltu $t2, $t4, $t2
; LA32-NEXT:    mulh.wu $t0, $a3, $t0
; LA32-NEXT:    add.w $t0, $t0, $t2
; LA32-NEXT:    add.w $t0, $t0, $t5
; LA32-NEXT:    mul.w $a3, $t1, $a3
; LA32-NEXT:    mulh.wu $t1, $t1, $a1
; LA32-NEXT:    add.w $a3, $t1, $a3
; LA32-NEXT:    add.w $a3, $a3, $t7
; LA32-NEXT:    add.w $a3, $t0, $a3
; LA32-NEXT:    add.w $a3, $a3, $s0
; LA32-NEXT:    mul.w $a2, $a2, $a5
; LA32-NEXT:    mul.w $a1, $a1, $a4
; LA32-NEXT:    st.w $a1, $a0, 16
; LA32-NEXT:    st.w $a2, $a0, 0
; LA32-NEXT:    st.w $a7, $a0, 20
; LA32-NEXT:    st.w $t3, $a0, 4
; LA32-NEXT:    st.w $t8, $a0, 24
; LA32-NEXT:    st.w $fp, $a0, 8
; LA32-NEXT:    st.w $a3, $a0, 28
; LA32-NEXT:    st.w $a6, $a0, 12
; LA32-NEXT:    ld.w $s0, $sp, 8 # 4-byte Folded Reload
; LA32-NEXT:    ld.w $fp, $sp, 12 # 4-byte Folded Reload
; LA32-NEXT:    addi.w $sp, $sp, 16
; LA32-NEXT:    ret
;
; LA64-LABEL: vmulwod_q_du_d:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    xvld $xr1, $a2, 0
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 3
; LA64-NEXT:    xvpickve2gr.d $a2, $xr0, 1
; LA64-NEXT:    xvpickve2gr.d $a3, $xr1, 1
; LA64-NEXT:    xvpickve2gr.d $a4, $xr1, 3
; LA64-NEXT:    srai.d $a5, $a4, 63
; LA64-NEXT:    srai.d $a6, $a3, 63
; LA64-NEXT:    mulh.du $a7, $a2, $a3
; LA64-NEXT:    mul.d $a6, $a2, $a6
; LA64-NEXT:    add.d $a6, $a7, $a6
; LA64-NEXT:    mulh.du $a7, $a1, $a4
; LA64-NEXT:    mul.d $a5, $a1, $a5
; LA64-NEXT:    add.d $a5, $a7, $a5
; LA64-NEXT:    mul.d $a2, $a2, $a3
; LA64-NEXT:    mul.d $a1, $a1, $a4
; LA64-NEXT:    st.d $a1, $a0, 16
; LA64-NEXT:    st.d $a2, $a0, 0
; LA64-NEXT:    st.d $a5, $a0, 24
; LA64-NEXT:    st.d $a6, $a0, 8
; LA64-NEXT:    ret
entry:
  %va = load <4 x i64>, ptr %a
  %vb = load <4 x i64>, ptr %b
  %vas = shufflevector <4 x i64> %va, <4 x i64> poison, <2 x i32> <i32 1, i32 3>
  %vbs = shufflevector <4 x i64> %vb, <4 x i64> poison, <2 x i32> <i32 1, i32 3>
  %vae = zext <2 x i64> %vas to <2 x i128>
  %vbe = sext <2 x i64> %vbs to <2 x i128>
  %mul = mul <2 x i128> %vae, %vbe
  store <2 x i128> %mul, ptr %res
  ret void
}

define void @vmulwev_h_bu_b_1(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwev_h_bu_b_1:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvmulwev.h.bu.b $xr0, $xr1, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <32 x i8>, ptr %a
  %vb = load <32 x i8>, ptr %b
  %vas = shufflevector <32 x i8> %va, <32 x i8> poison, <16 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14, i32 16, i32 18, i32 20, i32 22, i32 24, i32 26, i32 28, i32 30>
  %vbs = shufflevector <32 x i8> %vb, <32 x i8> poison, <16 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14, i32 16, i32 18, i32 20, i32 22, i32 24, i32 26, i32 28, i32 30>
  %vae = sext <16 x i8> %vas to <16 x i16>
  %vbe = zext <16 x i8> %vbs to <16 x i16>
  %mul = mul <16 x i16> %vae, %vbe
  store <16 x i16> %mul, ptr %res
  ret void
}

define void @vmulwev_w_hu_h_1(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwev_w_hu_h_1:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvmulwev.w.hu.h $xr0, $xr1, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <16 x i16>, ptr %a
  %vb = load <16 x i16>, ptr %b
  %vas = shufflevector <16 x i16> %va, <16 x i16> poison, <8 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14>
  %vbs = shufflevector <16 x i16> %vb, <16 x i16> poison, <8 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14>
  %vae = sext <8 x i16> %vas to <8 x i32>
  %vbe = zext <8 x i16> %vbs to <8 x i32>
  %mul = mul <8 x i32> %vae, %vbe
  store <8 x i32> %mul, ptr %res
  ret void
}

define void @vmulwev_d_wu_w_1(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwev_d_wu_w_1:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvmulwev.d.wu.w $xr0, $xr1, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <8 x i32>, ptr %a
  %vb = load <8 x i32>, ptr %b
  %vas = shufflevector <8 x i32> %va, <8 x i32> poison, <4 x i32> <i32 0, i32 2, i32 4, i32 6>
  %vbs = shufflevector <8 x i32> %vb, <8 x i32> poison, <4 x i32> <i32 0, i32 2, i32 4, i32 6>
  %vae = sext <4 x i32> %vas to <4 x i64>
  %vbe = zext <4 x i32> %vbs to <4 x i64>
  %mul = mul <4 x i64> %vae, %vbe
  store <4 x i64> %mul, ptr %res
  ret void
}

define void @vmulwev_q_du_d_1(ptr %res, ptr %a, ptr %b) nounwind {
; LA32-LABEL: vmulwev_q_du_d_1:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    addi.w $sp, $sp, -16
; LA32-NEXT:    st.w $fp, $sp, 12 # 4-byte Folded Spill
; LA32-NEXT:    st.w $s0, $sp, 8 # 4-byte Folded Spill
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 4
; LA32-NEXT:    xvld $xr1, $a2, 0
; LA32-NEXT:    xvpickve2gr.w $a2, $xr0, 0
; LA32-NEXT:    xvpickve2gr.w $a6, $xr0, 1
; LA32-NEXT:    xvpickve2gr.w $a7, $xr0, 5
; LA32-NEXT:    xvpickve2gr.w $a5, $xr1, 5
; LA32-NEXT:    xvpickve2gr.w $a3, $xr1, 4
; LA32-NEXT:    xvpickve2gr.w $t0, $xr1, 1
; LA32-NEXT:    xvpickve2gr.w $a4, $xr1, 0
; LA32-NEXT:    srai.w $t1, $a7, 31
; LA32-NEXT:    srai.w $t2, $a6, 31
; LA32-NEXT:    mulh.wu $t3, $a2, $a4
; LA32-NEXT:    mul.w $t4, $a6, $a4
; LA32-NEXT:    add.w $t3, $t4, $t3
; LA32-NEXT:    sltu $t4, $t3, $t4
; LA32-NEXT:    mulh.wu $t5, $a6, $a4
; LA32-NEXT:    add.w $t4, $t5, $t4
; LA32-NEXT:    mul.w $t5, $a2, $t0
; LA32-NEXT:    add.w $t3, $t5, $t3
; LA32-NEXT:    sltu $t5, $t3, $t5
; LA32-NEXT:    mulh.wu $t6, $a2, $t0
; LA32-NEXT:    add.w $t5, $t6, $t5
; LA32-NEXT:    add.w $t5, $t4, $t5
; LA32-NEXT:    mul.w $t6, $a6, $t0
; LA32-NEXT:    add.w $t7, $t6, $t5
; LA32-NEXT:    mul.w $t8, $a4, $t2
; LA32-NEXT:    add.w $fp, $t7, $t8
; LA32-NEXT:    sltu $s0, $fp, $t7
; LA32-NEXT:    sltu $t6, $t7, $t6
; LA32-NEXT:    sltu $t4, $t5, $t4
; LA32-NEXT:    mulh.wu $a6, $a6, $t0
; LA32-NEXT:    add.w $a6, $a6, $t4
; LA32-NEXT:    add.w $a6, $a6, $t6
; LA32-NEXT:    mulh.wu $t4, $a4, $t2
; LA32-NEXT:    add.w $t4, $t4, $t8
; LA32-NEXT:    mul.w $t0, $t0, $t2
; LA32-NEXT:    add.w $t0, $t4, $t0
; LA32-NEXT:    add.w $a6, $a6, $t0
; LA32-NEXT:    add.w $a6, $a6, $s0
; LA32-NEXT:    mulh.wu $t0, $a1, $a3
; LA32-NEXT:    mul.w $t2, $a7, $a3
; LA32-NEXT:    add.w $t0, $t2, $t0
; LA32-NEXT:    sltu $t2, $t0, $t2
; LA32-NEXT:    mulh.wu $t4, $a7, $a3
; LA32-NEXT:    add.w $t2, $t4, $t2
; LA32-NEXT:    mul.w $t4, $a1, $a5
; LA32-NEXT:    add.w $t0, $t4, $t0
; LA32-NEXT:    sltu $t4, $t0, $t4
; LA32-NEXT:    mulh.wu $t5, $a1, $a5
; LA32-NEXT:    add.w $t4, $t5, $t4
; LA32-NEXT:    add.w $t4, $t2, $t4
; LA32-NEXT:    mul.w $t5, $a7, $a5
; LA32-NEXT:    add.w $t6, $t5, $t4
; LA32-NEXT:    mul.w $t7, $a3, $t1
; LA32-NEXT:    add.w $t8, $t6, $t7
; LA32-NEXT:    sltu $s0, $t8, $t6
; LA32-NEXT:    sltu $t5, $t6, $t5
; LA32-NEXT:    sltu $t2, $t4, $t2
; LA32-NEXT:    mulh.wu $a7, $a7, $a5
; LA32-NEXT:    add.w $a7, $a7, $t2
; LA32-NEXT:    add.w $a7, $a7, $t5
; LA32-NEXT:    mulh.wu $t2, $a3, $t1
; LA32-NEXT:    add.w $t2, $t2, $t7
; LA32-NEXT:    mul.w $a5, $a5, $t1
; LA32-NEXT:    add.w $a5, $t2, $a5
; LA32-NEXT:    add.w $a5, $a7, $a5
; LA32-NEXT:    add.w $a5, $a5, $s0
; LA32-NEXT:    mul.w $a2, $a2, $a4
; LA32-NEXT:    mul.w $a1, $a1, $a3
; LA32-NEXT:    st.w $a1, $a0, 16
; LA32-NEXT:    st.w $a2, $a0, 0
; LA32-NEXT:    st.w $t0, $a0, 20
; LA32-NEXT:    st.w $t3, $a0, 4
; LA32-NEXT:    st.w $t8, $a0, 24
; LA32-NEXT:    st.w $fp, $a0, 8
; LA32-NEXT:    st.w $a5, $a0, 28
; LA32-NEXT:    st.w $a6, $a0, 12
; LA32-NEXT:    ld.w $s0, $sp, 8 # 4-byte Folded Reload
; LA32-NEXT:    ld.w $fp, $sp, 12 # 4-byte Folded Reload
; LA32-NEXT:    addi.w $sp, $sp, 16
; LA32-NEXT:    ret
;
; LA64-LABEL: vmulwev_q_du_d_1:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    xvld $xr1, $a2, 0
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 0
; LA64-NEXT:    xvpickve2gr.d $a2, $xr0, 2
; LA64-NEXT:    xvpickve2gr.d $a3, $xr1, 2
; LA64-NEXT:    xvpickve2gr.d $a4, $xr1, 0
; LA64-NEXT:    srai.d $a5, $a2, 63
; LA64-NEXT:    srai.d $a6, $a1, 63
; LA64-NEXT:    mulh.du $a7, $a1, $a4
; LA64-NEXT:    mul.d $a6, $a6, $a4
; LA64-NEXT:    add.d $a6, $a7, $a6
; LA64-NEXT:    mulh.du $a7, $a2, $a3
; LA64-NEXT:    mul.d $a5, $a5, $a3
; LA64-NEXT:    add.d $a5, $a7, $a5
; LA64-NEXT:    mul.d $a1, $a1, $a4
; LA64-NEXT:    mul.d $a2, $a2, $a3
; LA64-NEXT:    st.d $a2, $a0, 16
; LA64-NEXT:    st.d $a1, $a0, 0
; LA64-NEXT:    st.d $a5, $a0, 24
; LA64-NEXT:    st.d $a6, $a0, 8
; LA64-NEXT:    ret
entry:
  %va = load <4 x i64>, ptr %a
  %vb = load <4 x i64>, ptr %b
  %vas = shufflevector <4 x i64> %va, <4 x i64> poison, <2 x i32> <i32 0, i32 2>
  %vbs = shufflevector <4 x i64> %vb, <4 x i64> poison, <2 x i32> <i32 0, i32 2>
  %vae = sext <2 x i64> %vas to <2 x i128>
  %vbe = zext <2 x i64> %vbs to <2 x i128>
  %mul = mul <2 x i128> %vae, %vbe
  store <2 x i128> %mul, ptr %res
  ret void
}

define void @vmulwod_h_bu_b_1(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwod_h_bu_b_1:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvmulwod.h.bu.b $xr0, $xr1, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <32 x i8>, ptr %a
  %vb = load <32 x i8>, ptr %b
  %vas = shufflevector <32 x i8> %va, <32 x i8> poison, <16 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15, i32 17, i32 19, i32 21, i32 23, i32 25, i32 27, i32 29, i32 31>
  %vbs = shufflevector <32 x i8> %vb, <32 x i8> poison, <16 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15, i32 17, i32 19, i32 21, i32 23, i32 25, i32 27, i32 29, i32 31>
  %vae = sext <16 x i8> %vas to <16 x i16>
  %vbe = zext <16 x i8> %vbs to <16 x i16>
  %mul = mul <16 x i16> %vae, %vbe
  store <16 x i16> %mul, ptr %res
  ret void
}

define void @vmulwod_w_hu_h_1(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwod_w_hu_h_1:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvmulwod.w.hu.h $xr0, $xr1, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <16 x i16>, ptr %a
  %vb = load <16 x i16>, ptr %b
  %vas = shufflevector <16 x i16> %va, <16 x i16> poison, <8 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15>
  %vbs = shufflevector <16 x i16> %vb, <16 x i16> poison, <8 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15>
  %vae = sext <8 x i16> %vas to <8 x i32>
  %vbe = zext <8 x i16> %vbs to <8 x i32>
  %mul = mul <8 x i32> %vae, %vbe
  store <8 x i32> %mul, ptr %res
  ret void
}

define void @vmulwod_d_wu_w_1(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwod_d_wu_w_1:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvmulwod.d.wu.w $xr0, $xr1, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <8 x i32>, ptr %a
  %vb = load <8 x i32>, ptr %b
  %vas = shufflevector <8 x i32> %va, <8 x i32> poison, <4 x i32> <i32 1, i32 3, i32 5, i32 7>
  %vbs = shufflevector <8 x i32> %vb, <8 x i32> poison, <4 x i32> <i32 1, i32 3, i32 5, i32 7>
  %vae = sext <4 x i32> %vas to <4 x i64>
  %vbe = zext <4 x i32> %vbs to <4 x i64>
  %mul = mul <4 x i64> %vae, %vbe
  store <4 x i64> %mul, ptr %res
  ret void
}

define void @vmulwod_q_du_d_1(ptr %res, ptr %a, ptr %b) nounwind {
; LA32-LABEL: vmulwod_q_du_d_1:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    addi.w $sp, $sp, -16
; LA32-NEXT:    st.w $fp, $sp, 12 # 4-byte Folded Spill
; LA32-NEXT:    st.w $s0, $sp, 8 # 4-byte Folded Spill
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 6
; LA32-NEXT:    xvld $xr1, $a2, 0
; LA32-NEXT:    xvpickve2gr.w $a2, $xr0, 2
; LA32-NEXT:    xvpickve2gr.w $a6, $xr0, 3
; LA32-NEXT:    xvpickve2gr.w $a7, $xr0, 7
; LA32-NEXT:    xvpickve2gr.w $a5, $xr1, 7
; LA32-NEXT:    xvpickve2gr.w $a3, $xr1, 6
; LA32-NEXT:    xvpickve2gr.w $t0, $xr1, 3
; LA32-NEXT:    xvpickve2gr.w $a4, $xr1, 2
; LA32-NEXT:    srai.w $t1, $a7, 31
; LA32-NEXT:    srai.w $t2, $a6, 31
; LA32-NEXT:    mulh.wu $t3, $a2, $a4
; LA32-NEXT:    mul.w $t4, $a6, $a4
; LA32-NEXT:    add.w $t3, $t4, $t3
; LA32-NEXT:    sltu $t4, $t3, $t4
; LA32-NEXT:    mulh.wu $t5, $a6, $a4
; LA32-NEXT:    add.w $t4, $t5, $t4
; LA32-NEXT:    mul.w $t5, $a2, $t0
; LA32-NEXT:    add.w $t3, $t5, $t3
; LA32-NEXT:    sltu $t5, $t3, $t5
; LA32-NEXT:    mulh.wu $t6, $a2, $t0
; LA32-NEXT:    add.w $t5, $t6, $t5
; LA32-NEXT:    add.w $t5, $t4, $t5
; LA32-NEXT:    mul.w $t6, $a6, $t0
; LA32-NEXT:    add.w $t7, $t6, $t5
; LA32-NEXT:    mul.w $t8, $a4, $t2
; LA32-NEXT:    add.w $fp, $t7, $t8
; LA32-NEXT:    sltu $s0, $fp, $t7
; LA32-NEXT:    sltu $t6, $t7, $t6
; LA32-NEXT:    sltu $t4, $t5, $t4
; LA32-NEXT:    mulh.wu $a6, $a6, $t0
; LA32-NEXT:    add.w $a6, $a6, $t4
; LA32-NEXT:    add.w $a6, $a6, $t6
; LA32-NEXT:    mulh.wu $t4, $a4, $t2
; LA32-NEXT:    add.w $t4, $t4, $t8
; LA32-NEXT:    mul.w $t0, $t0, $t2
; LA32-NEXT:    add.w $t0, $t4, $t0
; LA32-NEXT:    add.w $a6, $a6, $t0
; LA32-NEXT:    add.w $a6, $a6, $s0
; LA32-NEXT:    mulh.wu $t0, $a1, $a3
; LA32-NEXT:    mul.w $t2, $a7, $a3
; LA32-NEXT:    add.w $t0, $t2, $t0
; LA32-NEXT:    sltu $t2, $t0, $t2
; LA32-NEXT:    mulh.wu $t4, $a7, $a3
; LA32-NEXT:    add.w $t2, $t4, $t2
; LA32-NEXT:    mul.w $t4, $a1, $a5
; LA32-NEXT:    add.w $t0, $t4, $t0
; LA32-NEXT:    sltu $t4, $t0, $t4
; LA32-NEXT:    mulh.wu $t5, $a1, $a5
; LA32-NEXT:    add.w $t4, $t5, $t4
; LA32-NEXT:    add.w $t4, $t2, $t4
; LA32-NEXT:    mul.w $t5, $a7, $a5
; LA32-NEXT:    add.w $t6, $t5, $t4
; LA32-NEXT:    mul.w $t7, $a3, $t1
; LA32-NEXT:    add.w $t8, $t6, $t7
; LA32-NEXT:    sltu $s0, $t8, $t6
; LA32-NEXT:    sltu $t5, $t6, $t5
; LA32-NEXT:    sltu $t2, $t4, $t2
; LA32-NEXT:    mulh.wu $a7, $a7, $a5
; LA32-NEXT:    add.w $a7, $a7, $t2
; LA32-NEXT:    add.w $a7, $a7, $t5
; LA32-NEXT:    mulh.wu $t2, $a3, $t1
; LA32-NEXT:    add.w $t2, $t2, $t7
; LA32-NEXT:    mul.w $a5, $a5, $t1
; LA32-NEXT:    add.w $a5, $t2, $a5
; LA32-NEXT:    add.w $a5, $a7, $a5
; LA32-NEXT:    add.w $a5, $a5, $s0
; LA32-NEXT:    mul.w $a2, $a2, $a4
; LA32-NEXT:    mul.w $a1, $a1, $a3
; LA32-NEXT:    st.w $a1, $a0, 16
; LA32-NEXT:    st.w $a2, $a0, 0
; LA32-NEXT:    st.w $t0, $a0, 20
; LA32-NEXT:    st.w $t3, $a0, 4
; LA32-NEXT:    st.w $t8, $a0, 24
; LA32-NEXT:    st.w $fp, $a0, 8
; LA32-NEXT:    st.w $a5, $a0, 28
; LA32-NEXT:    st.w $a6, $a0, 12
; LA32-NEXT:    ld.w $s0, $sp, 8 # 4-byte Folded Reload
; LA32-NEXT:    ld.w $fp, $sp, 12 # 4-byte Folded Reload
; LA32-NEXT:    addi.w $sp, $sp, 16
; LA32-NEXT:    ret
;
; LA64-LABEL: vmulwod_q_du_d_1:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    xvld $xr1, $a2, 0
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 1
; LA64-NEXT:    xvpickve2gr.d $a2, $xr0, 3
; LA64-NEXT:    xvpickve2gr.d $a3, $xr1, 3
; LA64-NEXT:    xvpickve2gr.d $a4, $xr1, 1
; LA64-NEXT:    srai.d $a5, $a2, 63
; LA64-NEXT:    srai.d $a6, $a1, 63
; LA64-NEXT:    mulh.du $a7, $a1, $a4
; LA64-NEXT:    mul.d $a6, $a6, $a4
; LA64-NEXT:    add.d $a6, $a7, $a6
; LA64-NEXT:    mulh.du $a7, $a2, $a3
; LA64-NEXT:    mul.d $a5, $a5, $a3
; LA64-NEXT:    add.d $a5, $a7, $a5
; LA64-NEXT:    mul.d $a1, $a1, $a4
; LA64-NEXT:    mul.d $a2, $a2, $a3
; LA64-NEXT:    st.d $a2, $a0, 16
; LA64-NEXT:    st.d $a1, $a0, 0
; LA64-NEXT:    st.d $a5, $a0, 24
; LA64-NEXT:    st.d $a6, $a0, 8
; LA64-NEXT:    ret
entry:
  %va = load <4 x i64>, ptr %a
  %vb = load <4 x i64>, ptr %b
  %vas = shufflevector <4 x i64> %va, <4 x i64> poison, <2 x i32> <i32 1, i32 3>
  %vbs = shufflevector <4 x i64> %vb, <4 x i64> poison, <2 x i32> <i32 1, i32 3>
  %vae = sext <2 x i64> %vas to <2 x i128>
  %vbe = zext <2 x i64> %vbs to <2 x i128>
  %mul = mul <2 x i128> %vae, %vbe
  store <2 x i128> %mul, ptr %res
  ret void
}
